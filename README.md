# deepseek_learning
一个快速学习deepseek v3模型以及r1强化学习的仓库，侧重与理解技术报告模型设计细节

# 一. v3
学习讲解资料来自[EZ.Encoder](https://www.youtube.com/@ez.encoder.academy)

模型代码来自[DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3)

## 模型结构
### MLA 多头低秩注意力
背景：传统模型在推理时使用k-v缓存进行加速，当文本越长需要的显存消耗越大（线性），MLA通过优化k-v缓存机制，降低推理需要的显存消耗，牺牲一点时间换空间。

传统k-v缓存生成方式：当预测得到下一个token，将其添加到历史输入进行前向传播，仅对该token生成k-v键值，如`k = h * Wk`，加入到历史token的k-v矩阵后进行前向计算，存储用于下一次计算

MLA优化原理：
  存储k-v变成存储一个长度更短的中间态`ch = h * Wd`, `Wd`对h进行降维。推理时，中间态度分别乘以一个矩阵得到k-v `k = ch * Wuk, v = ch * Wuv`，`Wu-`恢复每个token对应的向量长度。
  
  存储k-v缓存变成只存储一个更加低维度的`ch`，与传统方式相比，增加了一个低秩矩阵`Wd`，使用`Wuk Wuv`替换原来的`Wk Wv`。虽然多了一个`Wd`，但只存储`ch`极大减小显存

代码理解:
```python

```


### MOE + loss-free的专家均衡策略
MOE原理：MOE模块设置了共享专家和专精专家，每一个专家都是一个小的FFN，专精专家由一个路由gate(MLP + 分类头)进行分配，激活topk的专精专家进行前向传播

MOE训练弊端：gate的分配难以控制，可能导致分配偏向少数专精专家

MOE训练优化：
        在损失函数中增加专精专家的分配不均衡损失，这种方法会影响对其他损失函数的关注度。
        根据专精专家的历史分配次数，调整gate分类头输出的softmax分类logit，提高少分配专家的logit。loss-free，deepseek使用这种方法？
           
MOE代码理解
```python

```

loss-free的均衡策略开源的推理代码中未见

## 训练
### MTP 多token预测监督 
背景：基于decoder的问答模型只监督下一token的预测，但人在进行语言组织输出时通预见常不止一个文字，可能是一个词、一个句柄或一小段话，这更有利与语言组织的准确性。另一方面，下一token的输出出现偏离很容易会导致回答跑飞，考虑下下token，下下下token的可能性有利于当前token预测的纠偏。因此这种带有一定跨度的预见直观上可以进一个步提高监督的强度。

MTP经典方式：并行与串行
        论文
        
        deepseek采用的方式


MTP开源的推理代码未使用，只在训练时使用。增加一个简单实现


# 二. r1
r1的使用v3作为基础模型，训练增加了强化学习的策略
