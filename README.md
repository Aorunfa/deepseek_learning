# deepseek_learning
一个快速学习deepseek v3模型以及r1强化学习的仓库，侧重与理解技术报告模型设计细节

# 一. v3
技术报告与代码来自[DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3)

学习讲解资料来自[EZ.Encoder](https://www.youtube.com/@ez.encoder.academy)

## 代码说明
deepv3开源的推理代码兼容分布式多卡推理，将embedding, MLA, MOE模块中的线性层平分到所有的卡中，使得小显存卡也能组合起来运行。在阅读代码时需要注意`// word_size, dist.all_reduce()`。可以按照v3/model.py阅读代码，建议先跳过linear函数中量化与反量化操作


## 模型结构
### MLA 多头低秩注意力
背景：传统模型在推理时使用kv缓存进行加速，当文本越长需要的显存消耗越大（线性），MLA通过优化kv缓存机制，降低推理需要的显存消耗

传统kv缓存生成方式：当预测得到下一个token，将其添加到历史输入进行前向传播，仅对该token生成k-v键值，如`k = h * Wk`，加入到历史token的k-v矩阵后进行前向计算，存储用于下一次计算

MLA优化原理：存储kv变成存储一个更低维的中间态`kv = h * Wkv_a`, `Wkv_a`对h进行降维。 后续再由kv乘以一个矩阵得到v矩阵。

MLA其他创新：ROPE旋转位置编码按照维度进行解耦，最后计算softmax前对没有位置编码和有位置编码的查询结果进行相加。有点类似residual短连接效果


### MOE + loss-free的专家均衡策略
MOE原理：
MOE模块设置了共享专家和专精专家（routed_experts），每一个专家都是一个小的MLP，专精专家由一个路由gate进行分配(线性层，输出维度为model dim, 输出维度为专精专家数)，激活topk的专精专家进行前向传播。

MOE模块预设了分组过滤，先过滤组，再从组选择topk专家。

加总共享专家和专精专家的结果

MOE训练弊端：gate的分配难以控制，可能导致分配偏向少数专精专家

MOE训练优化：
        在损失函数中增加专精专家的分配不均衡损失，这种方法会影响对其他损失函数的关注度。
        根据专精专家的历史分配次数，调整gate分类头输出的softmax分类logit，提高少分配专家的logit。--- loss-free，deepseek使用这种方法？
           
## 训练
### MTP 多token预测监督 
背景：基于decoder的问答模型只监督下一token的预测，但人在进行语言组织输出时通预见常不止一个文字，可能是一个词、一个句柄或一小段话，这更有利与语言组织的准确性。另一方面，下一token的输出出现偏离很容易会导致回答跑飞，考虑下下token，下下下token的可能性有利于当前token预测的纠偏。因此这种带有一定跨度的预见直观上可以进一个步提高监督的强度。

MTP经典方式：并行与串行
        
        论文，简述差异
        
        deepseek采用的方式 [图]

        增加MTP的head进行推理加速


MTP开源的推理代码未使用，只在训练时使用。增加一个简单实现

### fp8量化训练
量化参数和linear模组

量化函数

反量化函数

### 加速通信方法

### r1知识提炼


# 二. r1
r1的使用v3作为基础模型，训练增加了强化学习的策略，使得在标签数据较少的情况下也能学习到知识并泛化
